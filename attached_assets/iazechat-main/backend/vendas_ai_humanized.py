"""
IA HUMANIZADA - WA SITE
Usa GPT-4o-mini com t√©cnica de prompt stuffing (instru√ß√µes em CADA mensagem)
SUPORTA: emergentintegrations OU OpenAI nativa (cada revenda pode ter sua chave)
"""
import logging
import os
from datetime import datetime, timezone
from typing import Tuple, List, Dict

logger = logging.getLogger(__name__)

class HumanizedVendasAI:
    """IA Humanizada que REALMENTE segue instru√ß√µes"""
    
    def __init__(self):
        logger.info("‚úÖ IA Humanizada inicializada (multi-key support)")
    
    async def get_response(
        self,
        user_message: str,
        session_id: str,
        instructions: str,
        db,
        custom_api_key: str = None  # üÜï Chave personalizada por revenda
    ) -> str:
        """
        Obt√©m resposta da IA REAL seguindo instru√ß√µes
        Suporta emergentintegrations OU OpenAI nativa
        üöÄ OTIMIZADO: Limita tamanho de instru√ß√µes para melhor performance
        üéØ NOVO: Verifica Base de Conhecimento CERTO|ERRADO primeiro
        """
        try:
            # üéØ PASSO 1: Verificar Base de Conhecimento CERTO|ERRADO
            from correct_wrong_knowledge import correct_wrong_service
            
            knowledge_match = await correct_wrong_service.search_knowledge(user_message, db)
            
            if knowledge_match:
                logger.info(f"üéØ Match na Base CERTO|ERRADO: {knowledge_match.get('question')}")
                
                # Formatar resposta usando apenas op√ß√µes CERTAS
                response_text = correct_wrong_service.format_response(knowledge_match, include_wrong=False)
                
                if response_text:
                    # Salvar no hist√≥rico
                    await self.save_message(session_id, "user", user_message, db)
                    await self.save_message(session_id, "assistant", response_text, db)
                    
                    logger.info(f"‚úÖ Resposta CERTA enviada (Base de Conhecimento)")
                    return response_text
            
            # Se n√£o achou na base CERTO|ERRADO, continuar com IA normal
            
            # Buscar hist√≥rico
            history = await self.get_history(session_id, db, limit=10)
            
            # Construir contexto do hist√≥rico
            history_text = ""
            if history:
                history_text = "\n\n[HIST√ìRICO DA CONVERSA]:\n"
                for msg in history[-6:]:  # √öltimas 6 mensagens
                    role = "Cliente" if msg.get('role') == 'user' else "Voc√™ (Juliana)"
                    history_text += f"{role}: {msg.get('content', '')}\n"
            
            # üöÄ OTIMIZA√á√ÉO CR√çTICA: Se instru√ß√µes s√£o muito grandes (>30k chars), limitar
            optimized_instructions = instructions
            
            if len(instructions) > 30000:
                logger.warning(f"‚ö†Ô∏è Instru√ß√µes muito grandes ({len(instructions)} chars) - Limitando a 25k para melhor performance")
                
                # Limitar a 25k chars (melhor performance mantendo qualidade)
                optimized_instructions = instructions[:25000]
                optimized_instructions += "\n\n[... restante das instru√ß√µes omitido para otimiza√ß√£o de performance ...]"
                
                logger.info(f"‚úÖ Instru√ß√µes otimizadas: {len(instructions)} ‚Üí {len(optimized_instructions)} chars")
            
            # üî• T√âCNICA: Colocar instru√ß√µes DENTRO da mensagem do usu√°rio
            stuffed_message = f"""[INSTRU√á√ïES QUE VOC√ä DEVE SEGUIR]:
{optimized_instructions}

{history_text}

[NOVA MENSAGEM DO CLIENTE]:
{user_message}

[IMPORTANTE]: 
- Voc√™ √â Juliana (n√£o diga que √© uma IA)
- Responda de forma NATURAL e HUMANIZADA
- Siga EXATAMENTE as regras das suas instru√ß√µes
- Seja DIRETA, n√£o gen√©rica
- Uma pergunta por vez"""
            
            system_prompt = "Voc√™ √© Juliana. Siga as instru√ß√µes fornecidas. Seja natural e humana."
            
            logger.info(f"ü§ñ Enviando para GPT-4o-mini | Session: {session_id} | Prompt size: {len(stuffed_message)} chars")
            
            # üîë DECIDIR QUAL BIBLIOTECA USAR
            api_key = custom_api_key or os.environ.get('OPENAI_API_KEY') or os.environ.get('EMERGENT_LLM_KEY')
            
            if not api_key:
                logger.error("‚ùå Nenhuma API key configurada!")
                return "Erro: API key n√£o configurada. Entre em contato com o suporte."
            
            # Detectar tipo de chave
            use_emergent = api_key.startswith('sk-emergent-')
            
            if use_emergent:
                # Usar emergentintegrations
                logger.info("üîß Usando emergentintegrations")
                response_text = await self._call_emergent(api_key, system_prompt, stuffed_message, session_id)
            else:
                # Usar OpenAI nativa
                logger.info("üîß Usando OpenAI SDK nativa")
                response_text = await self._call_openai(api_key, system_prompt, stuffed_message)
            
            logger.info(f"‚úÖ GPT respondeu ({len(response_text)} chars): {response_text[:100]}...")
            
            # Salvar no hist√≥rico
            await self.save_message(session_id, "user", user_message, db)
            await self.save_message(session_id, "assistant", response_text, db)
            
            return response_text
            
        except Exception as e:
            logger.error(f"‚ùå Erro ao obter resposta da IA: {e}")
            import traceback
            logger.error(f"‚ùå Traceback completo: {traceback.format_exc()}")
            return "Desculpe, tive um problema t√©cnico. Pode repetir?"
    
    async def _call_emergent(self, api_key: str, system_prompt: str, user_message: str, session_id: str) -> str:
        """Chamar usando emergentintegrations com timeout"""
        try:
            from emergentintegrations.llm.chat import LlmChat, UserMessage
            import asyncio
            
            chat = LlmChat(
                api_key=api_key,
                session_id=session_id,
                system_message=system_prompt,
                initial_messages=None
            ).with_model("openai", "gpt-4o-mini")
            
            message = UserMessage(text=user_message)
            
            # üöÄ OTIMIZA√á√ÉO: Adicionar timeout de 15 segundos
            try:
                response = await asyncio.wait_for(
                    chat.send_message(message),
                    timeout=15.0
                )
            except asyncio.TimeoutError:
                logger.error("‚è±Ô∏è Timeout ao chamar IA (15s)")
                return "Desculpe, estou demorando muito para responder. Pode tentar novamente?"
            
            if hasattr(response, 'to_text'):
                return response.to_text()
            elif hasattr(response, 'text'):
                return response.text
            else:
                return str(response)
        except Exception as e:
            logger.error(f"‚ùå Erro emergent: {e}")
            raise
    
    async def _call_openai(self, api_key: str, system_prompt: str, user_message: str) -> str:
        """Chamar usando OpenAI SDK nativa"""
        try:
            from openai import AsyncOpenAI
            
            client = AsyncOpenAI(api_key=api_key)
            
            # üöÄ OTIMIZA√á√ÉO: Reduzir max_tokens para respostas mais r√°pidas
            response = await client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_message}
                ],
                temperature=0.9,
                max_tokens=300,  # üöÄ Reduzido de 500 para 300 (respostas mais r√°pidas)
                timeout=15.0  # üöÄ Timeout de 15 segundos
            )
            
            return response.choices[0].message.content
            
        except Exception as e:
            logger.error(f"‚ùå Erro OpenAI: {e}")
            raise
    
    async def save_message(self, session_id: str, role: str, content: str, db):
        """Salva mensagem no hist√≥rico"""
        try:
            doc = {
                "session_id": session_id,
                "role": role,
                "content": content,
                "timestamp": datetime.now(timezone.utc).isoformat()
            }
            await db.ai_conversation_memory.insert_one(doc)
            logger.info(f"üíæ Salvo: {role}")
        except Exception as e:
            logger.error(f"‚ùå Erro ao salvar: {e}")
    
    async def get_history(self, session_id: str, db, limit: int = 20) -> List[Dict]:
        """Recupera hist√≥rico"""
        try:
            msgs = await db.ai_conversation_memory.find(
                {"session_id": session_id}
            ).sort("timestamp", -1).limit(limit).to_list(length=None)
            
            msgs.reverse()
            return msgs
        except Exception as e:
            logger.error(f"‚ùå Erro ao buscar hist√≥rico: {e}")
            return []


# Inst√¢ncia global
humanized_vendas_ai = HumanizedVendasAI()
